{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\danxing\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.461 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按词频降序排序的训练集列表: [',', '，', '？', '的', '！', ' ', '：', '了', '是', '你', '“', '”', '有', '吗', '在', '中国', '如何', '什么', '怎么', '和', '为什么', '人', '-', '都', '不', '被', '》', '《', '对', '我', '美国', '、', '会', '年', '大', '看', '这', '游戏', '手机', '上', '能', '农村', '好', '最', '与', '上联', '新', '下联', '后', '5', '要', '还', '中', '就', '3', '汽车', '将', '哪些', '世界', '一个', '网友', '月', '2018', '让', '去', '他', '—', '谁', '日本', '俄罗斯', '又', '如果', '2', '万', '却', '王者', '买', '到', '说', '多', '荣耀', '也', '英雄', '可以', '个', '没有', '还是', '小', '·', '用', '4', '来', '做', '现在', '为', '房价', '10', '小米', '伊朗', '车']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "9\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      news_finance       0.63      0.48      0.55      8079\n",
      "        news_story       0.53      0.47      0.50      1912\n",
      "       news_travel       0.50      0.36      0.41      6379\n",
      "          news_edu       0.62      0.47      0.54      8069\n",
      "     news_military       0.64      0.44      0.52      7416\n",
      "         news_game       0.62      0.44      0.51      8859\n",
      "  news_agriculture       0.56      0.32      0.41      5754\n",
      "        news_house       0.76      0.51      0.62      5334\n",
      "       news_sports       0.79      0.62      0.70     11291\n",
      "          news_car       0.78      0.61      0.69     10859\n",
      "         news_tech       0.22      0.73      0.34     12273\n",
      "             stock       0.00      0.00      0.00        94\n",
      "news_entertainment       0.60      0.51      0.55     11944\n",
      "      news_culture       0.50      0.27      0.35      8352\n",
      "        news_world       0.49      0.33      0.39      8192\n",
      "\n",
      "          accuracy                           0.49    114807\n",
      "         macro avg       0.55      0.44      0.47    114807\n",
      "      weighted avg       0.59      0.49      0.51    114807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "import random\n",
    "import jieba\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import joblib\n",
    "# 手写拉普拉斯修正的朴素贝叶斯\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    函数说明:中文文本处理\n",
    "    Parameters:\n",
    "        path - 文本存放的路径\n",
    "        test_size - 测试集占比，默认占所有数据集的百分之20\n",
    "    Returns:\n",
    "        all_words_list - 按词频降序排序的训练集列表\n",
    "        train_data_list - 训练集列表\n",
    "        test_data_list - 测试集列表\n",
    "        train_class_list - 训练集标签列表\n",
    "        test_class_list - 测试集标签列表\n",
    "\"\"\"\n",
    "def TextProcessing(path, test_size=0.3):\n",
    "#    folder_list = os.listdir(folder_path)  # 查看folder_path下的文件\n",
    "    data_list = []  # 数据集数据\n",
    "    class_list = []  # 数据集类别\n",
    "    with open(path, 'r', encoding='utf-8') as f:  # 打开txt文件\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split(\"_!_\") # 去除\"_!_\"\n",
    "            # print(line)\n",
    "            if (len(line) >= 5):\n",
    "                strr = line[3] + line[4]    # 有标题和关键词\n",
    "            else:\n",
    "                strr = line[3]  # 只有标题\n",
    "            word_cut = jieba.cut(strr, cut_all=False)  # 精简模式，返回一个可迭代的generator\n",
    "            word_list = list(word_cut)  # generator转换为list\n",
    "            data_list.append(word_list)\n",
    "            class_list.append(line[2])\n",
    "\n",
    "    data_class_list = list(zip(data_list, class_list))  # zip压缩合并，将数据与标签对应压缩\n",
    "    random.shuffle(data_class_list)  # 将data_class_list乱序\n",
    "    index = int(len(data_class_list) * test_size) + 1  # 训练集和测试集切分的索引值\n",
    "    train_list = data_class_list[index:]  # 训练集\n",
    "    test_list = data_class_list[:index]  # 测试集\n",
    "    train_data_list, train_class_list = zip(*train_list)  # 训练集解压缩\n",
    "    test_data_list, test_class_list = zip(*test_list)  # 测试集解压缩\n",
    "\n",
    "    all_words_dict = {}  # 统计训练集词频\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_words_dict.keys():\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "\n",
    "    # 根据键的值倒序排序\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f: f[1], reverse=True)\n",
    "    all_words_list, all_words_nums = zip(*all_words_tuple_list)  # 解压缩\n",
    "    all_words_list = list(all_words_list)  # 转换成列表\n",
    "    print(\"按词频降序排序的训练集列表:\",all_words_list[:100])\n",
    "    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:读取文件里的内容，并去重\n",
    "Parameters:\n",
    "    words_file - 文件路径\n",
    "Returns:\n",
    "    words_set - 读取的内容的set集合\n",
    "\"\"\"\n",
    "def MakeWordsSet(words_file):\n",
    "    words_set = set()  # 创建set集合\n",
    "    with open(words_file, 'r', encoding='utf-8') as f:  # 打开文件\n",
    "        for line in f.readlines():  # 一行一行读取\n",
    "            word = line.strip()  # 去回车\n",
    "            if len(word) > 0:  # 有文本，则添加到words_set中\n",
    "                words_set.add(word)\n",
    "    return words_set  # 返回处理结果\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:文本特征选取\n",
    "Parameters:\n",
    "    all_words_list - 训练集所有文本列表\n",
    "    deleteN - 删除词频最高的deleteN个词\n",
    "    stopwords_set - 指定的结束语\n",
    "Returns:\n",
    "    feature_words - 特征集\n",
    "\"\"\"\n",
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    feature_words = []  # 特征列表\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 1000:  # feature_words的维度为1000\n",
    "            break\n",
    "            # 如果这个词不是数字，并且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词\n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1 < len(all_words_list[t]) < 5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "        n += 1\n",
    "    return feature_words\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:根据feature_words将文本向量化\n",
    "Parameters:\n",
    "    train_data_list - 训练集\n",
    "    test_data_list - 测试集\n",
    "    feature_words - 特征集\n",
    "Returns:\n",
    "    train_feature_list - 训练集向量化列表\n",
    "    test_feature_list - 测试集向量化列表\n",
    "\"\"\"\n",
    "def TextFeatures(train_data_list, test_data_list, feature_words):\n",
    "    def text_features(text, feature_words):  # 出现在特征集中，则置1\n",
    "        text_words = set(text)\n",
    "        features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        return features\n",
    "\n",
    "    train_feature_list = [text_features(text, feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words) for text in test_data_list]\n",
    "    # for features in train_feature_list:\n",
    "    #     for index in range(len(features)):\n",
    "    #         features[index]=str(index)+\"_\"+str(features[index])\n",
    "    # for features in test_feature_list:\n",
    "    #     for index in range(len(features)):\n",
    "    #         features[index]=str(index)+\"_\"+str(features[index])\n",
    "\n",
    "    return train_feature_list, test_feature_list  # 返回结果\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:新闻分类器\n",
    "Parameters:\n",
    "    train_feature_list - 训练集向量化的特征文本\n",
    "    test_feature_list - 测试集向量化的特征文本\n",
    "    train_class_list - 训练集分类标签\n",
    "    test_class_list - 测试集分类标签\n",
    "Returns:\n",
    "    test_accuracy - 分类器精度\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 文本预处理\n",
    "    folder_path = \"./toutiao_cat_data.txt\"  # 训练集存放地址\n",
    "    all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path,test_size=0.3)\n",
    "    # 生成stopwords_set\n",
    "    stopwords_file = './stopwords_cn.txt'\n",
    "    stopwords_set = MakeWordsSet(stopwords_file)\n",
    "\n",
    "    test_accuracy_list = []\n",
    "\n",
    "    clf=MultinomialNB()\n",
    "\n",
    "    id2class=['news_finance', 'news_story', 'news_travel', 'news_edu', 'news_military', 'news_game', 'news_agriculture', 'news_house', 'news_sports', 'news_car', 'news_tech', 'stock', 'news_entertainment', 'news_culture', 'news_world']\n",
    "\n",
    "    class2id = {}\n",
    "    index = 0\n",
    "    for i in id2class:\n",
    "        class2id[i] = index\n",
    "        index = index + 1\n",
    "    # print(id2class)\n",
    "    train_class_list=[class2id[i] for i in train_class_list]\n",
    "    test_class_list = [class2id[i] for i in test_class_list]\n",
    "    feature_words = words_dict(all_words_list, 450, stopwords_set)\n",
    "    a = np.array(feature_words)\n",
    "    np.save(\"./feature_words.npy\", a)  # 保存为.npy格式\n",
    "\n",
    "\n",
    "    #print(feature_words)\n",
    "    train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words)\n",
    "    print(train_feature_list[0])\n",
    "    print(train_class_list[0])\n",
    "    clf.fit(train_feature_list,train_class_list)\n",
    "\n",
    "    joblib.dump(clf, \"./bayes.m\")\n",
    "\n",
    "    predict_y=clf.predict(test_feature_list)\n",
    "    print(classification_report(test_class_list, predict_y, target_names=id2class))\n",
    "    # acc = TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list,c1)\n",
    "    # #print(c1.cc)\n",
    "    # #print(c1.fc)\n",
    "    # print(\"acc:\",acc)\n",
    "    # #print(\"predict lable:\",lable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
