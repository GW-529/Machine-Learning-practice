{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      news_finance       0.81      0.74      0.78      8157\n",
      "        news_story       0.65      0.82      0.73      1892\n",
      "       news_travel       0.74      0.81      0.77      6404\n",
      "          news_edu       0.84      0.85      0.85      8142\n",
      "     news_military       0.83      0.82      0.83      7502\n",
      "         news_game       0.85      0.82      0.84      8690\n",
      "  news_agriculture       0.82      0.80      0.81      5693\n",
      "        news_house       0.85      0.85      0.85      5291\n",
      "       news_sports       0.94      0.91      0.93     11330\n",
      "          news_car       0.89      0.89      0.89     10824\n",
      "         news_tech       0.72      0.81      0.77     12416\n",
      "             stock       0.00      0.00      0.00       113\n",
      "news_entertainment       0.86      0.88      0.87     11815\n",
      "      news_culture       0.81      0.75      0.78      8403\n",
      "        news_world       0.80      0.73      0.76      8135\n",
      "\n",
      "          accuracy                           0.83    114807\n",
      "         macro avg       0.76      0.77      0.76    114807\n",
      "      weighted avg       0.83      0.83      0.83    114807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "import random\n",
    "import jieba\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import joblib\n",
    "# 手写拉普拉斯修正的朴素贝叶斯\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    函数说明:中文文本处理。将今日头条文本数据切割，保留分类名称，新闻字符串（仅含标题），新闻关键词，将新闻字符串切割成词和关键词组成。存储分类名称和词。\n",
    "    Parameters:\n",
    "        path - 文本存放的路径\n",
    "        test_size - 测试集占比，默认占所有数据集的百分之30\n",
    "    Returns:\n",
    "        all_words_list - 按词频降序排序的训练集列表\n",
    "        train_data_list - 训练集列表\n",
    "        test_data_list - 测试集列表\n",
    "        train_class_list - 训练集标签列表\n",
    "        test_class_list - 测试集标签列表\n",
    "\"\"\"\n",
    "def TextProcessing(path, test_size=0.3):\n",
    "#    folder_list = os.listdir(folder_path)  # 查看folder_path下的文件\n",
    "    data_list = []  # 数据集数据\n",
    "    class_list = []  # 数据集类别\n",
    "    with open(path, 'r', encoding='utf-8') as f:  # 打开txt文件\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split(\"_!_\") # 去除\"_!_\"\n",
    "            # print(line)\n",
    "            if (len(line) >= 5):\n",
    "                strr = line[3] + line[4]    # 有标题和关键词\n",
    "            else:\n",
    "                strr = line[3]  # 只有标题\n",
    "            word_cut = jieba.cut(strr, cut_all=False)  # 精简模式，返回一个可迭代的generator\n",
    "            word_list = list(word_cut)  # generator转换为list\n",
    "            word_filtered_list = []\n",
    "            for v in word_list:\n",
    "                if all('\\u4e00' <= char <= '\\u9fff' for char in v) and 1 < len(v) < 5:\n",
    "                    word_filtered_list.append(v)\n",
    "            data_list.append(word_filtered_list)\n",
    "            class_list.append(line[2])\n",
    "    \n",
    "    data_class_list = list(zip(data_list, class_list))  # zip压缩合并，将数据与标签对应压缩\n",
    "    random.shuffle(data_class_list)  # 将data_class_list乱序\n",
    "    index = int(len(data_class_list) * test_size) + 1  # 训练集和测试集切分的索引值\n",
    "    train_list = data_class_list[index:]  # 训练集\n",
    "    test_list = data_class_list[:index]  # 测试集\n",
    "    train_data_list, train_class_list = zip(*train_list)  # 训练集解压缩\n",
    "    test_data_list, test_class_list = zip(*test_list)  # 测试集解压缩\n",
    "    \n",
    "    all_words_dict = {}  # 统计训练集词频\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_words_dict.keys():\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "\n",
    "    # 根据键的值倒序排序\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f: f[1], reverse=True)\n",
    "    all_words_list, all_words_nums = zip(*all_words_tuple_list)  # 解压缩\n",
    "    all_words_list = list(all_words_list)  # 转换成列表\n",
    "    \n",
    "    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:读取停用词文件里的内容，保存到列表\n",
    "Parameters:\n",
    "    words_file - 文件路径\n",
    "Returns:\n",
    "    words_set - 读取的内容的set集合\n",
    "\"\"\"\n",
    "def MakeWordsSet(words_file):\n",
    "    words_set = set()  # 创建set集合\n",
    "    with open(words_file, 'r', encoding='utf-8') as f:  # 打开文件\n",
    "        for line in f.readlines():  # 一行一行读取\n",
    "            word = line.strip()  # 去回车\n",
    "            if len(word) > 0:  # 有文本，则添加到words_set中\n",
    "                words_set.add(word)\n",
    "    return words_set  # 返回处理结果\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:文本特征选取。读取所有词列表\n",
    "Parameters:\n",
    "    all_words_list - 训练集所有文本列表\n",
    "    deleteN - 删除词频最高的deleteN个词\n",
    "    stopwords_set - 指定的结束语\n",
    "Returns:\n",
    "    feature_words - 特征集\n",
    "\"\"\"\n",
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    feature_num = 16000\n",
    "    feature_words = []  # 特征列表\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > feature_num:  # feature_words的维度\n",
    "            break\n",
    "            # 如果这个词不是数字，并且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词\n",
    "        if all_words_list[t] not in stopwords_set:\n",
    "            feature_words.append(all_words_list[t])\n",
    "        n += 1\n",
    "    return feature_words\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:根据feature_words将文本向量化\n",
    "Parameters:\n",
    "    train_data_list - 训练集\n",
    "    test_data_list - 测试集\n",
    "    feature_words - 特征集\n",
    "Returns:\n",
    "    train_feature_list - 训练集向量化列表\n",
    "    test_feature_list - 测试集向量化列表\n",
    "\"\"\"\n",
    "def TextFeatures(data_list, feature_words):\n",
    "    def text_features(text, feature_words):  # 出现在特征集中，则置1\n",
    "        text_words = set(text)\n",
    "        features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        return features\n",
    "\n",
    "    feature_list = [text_features(text, feature_words) for text in data_list]\n",
    "\n",
    "    return feature_list # 返回结果\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:新闻分类器\n",
    "Parameters:\n",
    "    train_feature_list - 训练集向量化的特征文本\n",
    "    test_feature_list - 测试集向量化的特征文本\n",
    "    train_class_list - 训练集分类标签\n",
    "    test_class_list - 测试集分类标签\n",
    "Returns:\n",
    "    test_accuracy - 分类器精度\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 文本预处理，获取训练集和测试集数据\n",
    "    folder_path = \"./toutiao_cat_data.txt\"\n",
    "    all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size=0.3)\n",
    "    \n",
    "    # 生成停用词集合\n",
    "    stopwords_file = './stopwords_cn.txt'\n",
    "    stopwords_set = MakeWordsSet(stopwords_file)\n",
    "    \n",
    "    # 选择特征词\n",
    "    feature_words = words_dict(all_words_list, 100, stopwords_set)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 初始化朴素贝叶斯分类器\n",
    "    clf = MultinomialNB()\n",
    "    \n",
    "    id2class=['news_finance', 'news_story', 'news_travel', 'news_edu', 'news_military', 'news_game', 'news_agriculture', 'news_house', 'news_sports', 'news_car', 'news_tech', 'stock', 'news_entertainment', 'news_culture', 'news_world']\n",
    "\n",
    "    class2id = {}\n",
    "    index = 0\n",
    "    for i in id2class:\n",
    "        class2id[i] = index\n",
    "        index = index + 1\n",
    "    # print(id2class)\n",
    "    train_class_list=[class2id[i] for i in train_class_list]\n",
    "    test_class_list = [class2id[i] for i in test_class_list]\n",
    "\n",
    "    # 定义批次大小\n",
    "    batch_size = 1000\n",
    "    \n",
    "    # 训练模型\n",
    "    for i in range(0, len(train_data_list), batch_size):\n",
    "        # 文本特征向量化\n",
    "        train_feature_list = TextFeatures(train_data_list[i:i + batch_size], feature_words)\n",
    "        batch_X = train_feature_list\n",
    "        batch_y = train_class_list[i:i + batch_size]\n",
    "        \n",
    "        clf.partial_fit(batch_X, batch_y, classes=np.unique(train_class_list))\n",
    "    \n",
    "    # 保存模型\n",
    "    joblib.dump(clf, \"./bayes_model.pkl\")\n",
    "    \n",
    "    # 加载模型\n",
    "    clf_loaded = joblib.load(\"./bayes_model.pkl\")\n",
    "\n",
    "    # 预测并输出分类报告\n",
    "    # test_feature_list = TextFeatures(test_data_list, feature_words)\n",
    "    # predict_y = clf_loaded.predict(test_feature_list)\n",
    "    # print(classification_report(test_class_list, predict_y))\n",
    "\n",
    "    predict_y = []\n",
    "    for i in range(0, len(test_data_list), batch_size):\n",
    "        test_feature_list = TextFeatures(test_data_list[i:i + batch_size], feature_words)\n",
    "        batch_features = test_feature_list\n",
    "        batch_predictions = clf_loaded.predict(batch_features)\n",
    "        predict_y.extend(batch_predictions)\n",
    "\n",
    "    print(classification_report(test_class_list, predict_y, target_names=id2class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
