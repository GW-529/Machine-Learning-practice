{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      news_finance       0.56      0.40      0.46      7982\n",
      "        news_story       0.97      0.01      0.03      1915\n",
      "       news_travel       0.53      0.13      0.21      6423\n",
      "          news_edu       0.40      0.46      0.43      8120\n",
      "     news_military       0.47      0.50      0.48      7441\n",
      "         news_game       0.46      0.43      0.45      8865\n",
      "  news_agriculture       0.60      0.17      0.27      5788\n",
      "        news_house       0.79      0.25      0.39      5312\n",
      "       news_sports       0.82      0.59      0.68     11381\n",
      "          news_car       0.56      0.61      0.58     10670\n",
      "         news_tech       0.42      0.49      0.46     12517\n",
      "             stock       0.00      0.00      0.00        89\n",
      "news_entertainment       0.53      0.44      0.48     11899\n",
      "      news_culture       0.21      0.69      0.33      8368\n",
      "        news_world       0.33      0.29      0.31      8037\n",
      "\n",
      "          accuracy                           0.44    114807\n",
      "         macro avg       0.51      0.37      0.37    114807\n",
      "      weighted avg       0.52      0.44      0.44    114807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "import random\n",
    "import jieba\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "# 手写拉普拉斯修正的朴素贝叶斯\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    函数说明:中文文本处理\n",
    "    Parameters:\n",
    "        path - 文本存放的路径\n",
    "        test_size - 测试集占比，默认占所有数据集的百分之20\n",
    "    Returns:\n",
    "        all_words_list - 按词频降序排序的训练集列表\n",
    "        train_data_list - 训练集列表\n",
    "        test_data_list - 测试集列表\n",
    "        train_class_list - 训练集标签列表\n",
    "        test_class_list - 测试集标签列表\n",
    "\"\"\"\n",
    "def TextProcessing(path, test_size=0.2):\n",
    "#    folder_list = os.listdir(folder_path)  # 查看folder_path下的文件\n",
    "    data_list = []  # 数据集数据\n",
    "    class_list = []  # 数据集类别\n",
    "    with open(path, 'r', encoding='utf-8') as f:  # 打开txt文件\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split(\"_!_\")\n",
    "            # print(line)\n",
    "            if (len(line) >= 5):\n",
    "                strr = line[3] + line[4]\n",
    "            else:\n",
    "                strr = line[3]\n",
    "            word_cut = jieba.cut(strr, cut_all=False)  # 精简模式，返回一个可迭代的generator\n",
    "            word_list = list(word_cut)  # generator转换为list\n",
    "            data_list.append(word_list)\n",
    "            class_list.append(line[2])\n",
    "\n",
    "    data_class_list = list(zip(data_list, class_list))  # zip压缩合并，将数据与标签对应压缩\n",
    "    random.shuffle(data_class_list)  # 将data_class_list乱序\n",
    "    index = int(len(data_class_list) * test_size) + 1  # 训练集和测试集切分的索引值\n",
    "    train_list = data_class_list[index:]  # 训练集\n",
    "    test_list = data_class_list[:index]  # 测试集\n",
    "    train_data_list, train_class_list = zip(*train_list)  # 训练集解压缩\n",
    "    test_data_list, test_class_list = zip(*test_list)  # 测试集解压缩\n",
    "\n",
    "    all_words_dict = {}  # 统计训练集词频\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_words_dict.keys():\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "\n",
    "    # 根据键的值倒序排序\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f: f[1], reverse=True)\n",
    "    all_words_list, all_words_nums = zip(*all_words_tuple_list)  # 解压缩\n",
    "    all_words_list = list(all_words_list)  # 转换成列表\n",
    "    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:读取文件里的内容，并去重\n",
    "Parameters:\n",
    "    words_file - 文件路径\n",
    "Returns:\n",
    "    words_set - 读取的内容的set集合\n",
    "\"\"\"\n",
    "def MakeWordsSet(words_file):\n",
    "    words_set = set()  # 创建set集合\n",
    "    with open(words_file, 'r', encoding='utf-8') as f:  # 打开文件\n",
    "        for line in f.readlines():  # 一行一行读取\n",
    "            word = line.strip()  # 去回车\n",
    "            if len(word) > 0:  # 有文本，则添加到words_set中\n",
    "                words_set.add(word)\n",
    "    return words_set  # 返回处理结果\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:文本特征词选取\n",
    "Parameters:\n",
    "    all_words_list - 训练集所有文本列表\n",
    "    deleteN - 删除词频最高的deleteN个词\n",
    "    stopwords_set - 指定的结束语\n",
    "Returns:\n",
    "    feature_words - 特征集\n",
    "\"\"\"\n",
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    feature_words = []  # 特征列表\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 10000:  # feature_words的维度为1000\n",
    "            break\n",
    "            # 如果这个词不是数字，并且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词\n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1 < len(all_words_list[t]) < 5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "        n += 1\n",
    "    return feature_words\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:根据feature_words将文本向量化\n",
    "Parameters:\n",
    "    train_data_list - 训练集\n",
    "    test_data_list - 测试集\n",
    "    feature_words - 特征集\n",
    "Returns:\n",
    "    train_feature_list - 训练集向量化列表\n",
    "    test_feature_list - 测试集向量化列表\n",
    "\"\"\"\n",
    "def TextFeatures(data_list, feature_words):\n",
    "    def text_features(text, feature_words):  # 出现在特征集中，则置1\n",
    "        text_words = set(text)\n",
    "        features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        return features\n",
    "\n",
    "    feature_list = [text_features(text, feature_words) for text in data_list]\n",
    "\n",
    "    return feature_list # 返回结果\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:新闻分类器\n",
    "Parameters:\n",
    "    train_feature_list - 训练集向量化的特征文本\n",
    "    test_feature_list - 测试集向量化的特征文本\n",
    "    train_class_list - 训练集分类标签\n",
    "    test_class_list - 测试集分类标签\n",
    "Returns:\n",
    "    test_accuracy - 分类器精度\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 文本预处理，获取训练集和测试集数据\n",
    "    folder_path = \"./toutiao_cat_data.txt\"\n",
    "    all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size=0.3)\n",
    "    \n",
    "    # 生成停用词集合\n",
    "    stopwords_file = './stopwords_cn.txt'\n",
    "    stopwords_set = MakeWordsSet(stopwords_file)\n",
    "    \n",
    "    # 选择特征词\n",
    "    feature_words = words_dict(all_words_list, 450, stopwords_set)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 初始化朴素贝叶斯分类器\n",
    "    clf = LogisticRegression()\n",
    "    \n",
    "    id2class=['news_finance', 'news_story', 'news_travel', 'news_edu', 'news_military', 'news_game', 'news_agriculture', 'news_house', 'news_sports', 'news_car', 'news_tech', 'stock', 'news_entertainment', 'news_culture', 'news_world']\n",
    "\n",
    "    class2id = {}\n",
    "    index = 0\n",
    "    for i in id2class:\n",
    "        class2id[i] = index\n",
    "        index = index + 1\n",
    "    # print(id2class)\n",
    "    train_class_list=[class2id[i] for i in train_class_list]\n",
    "    test_class_list = [class2id[i] for i in test_class_list]\n",
    "\n",
    "    # 定义批次大小\n",
    "    batch_size = 1000\n",
    "    \n",
    "    # 训练模型\n",
    "    for i in range(0, len(train_data_list), batch_size):\n",
    "        # 文本特征向量化\n",
    "        train_feature_list = TextFeatures(train_data_list[i:i + batch_size], feature_words)\n",
    "        batch_X = train_feature_list\n",
    "        batch_y = train_class_list[i:i + batch_size]\n",
    "        \n",
    "        clf.fit(batch_X, batch_y)\n",
    "    \n",
    "    # 保存模型\n",
    "    joblib.dump(clf, \"./logr_model.pkl\")\n",
    "    \n",
    "    # 加载模型\n",
    "    clf_loaded = joblib.load(\"./logr_model.pkl\")\n",
    "\n",
    "    predict_y = []\n",
    "    for i in range(0, len(test_data_list), batch_size):\n",
    "        test_feature_list = TextFeatures(test_data_list[i:i + batch_size], feature_words)\n",
    "        batch_features = test_feature_list\n",
    "        batch_predictions = clf_loaded.predict(batch_features)\n",
    "        predict_y.extend(batch_predictions)\n",
    "\n",
    "    print(classification_report(test_class_list, predict_y, target_names=id2class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
